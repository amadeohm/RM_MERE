# ğŸ§  AI-Driven Data Analysis Benchmark (Gemini, ChatGPT, DeepSeek vs. Human)

This repository contains the code, data, and prompts used in the comparative study evaluating **AI-generated data analyses** versus **human-led analyses** across *tabular* and *text* datasets.  
The project investigates the **quality, methodological bias, and correctness** of analyses produced by three large language models (Gemini, ChatGPT, and DeepSeek) compared to a human benchmark.

---

## ğŸ“ Repository Structure

```
.
â”œâ”€â”€ Codes
â”‚   â”œâ”€â”€ Tabular
â”‚   â”‚   â”œâ”€â”€ 01_introducing_missing_values.ipynb     # Notebook introducing missing values into the dataset
â”‚   â”‚   â”œâ”€â”€ 02_human_analysis.ipynb                 # Human analystâ€™s step-by-step tabular data analysis
â”‚   â”‚   â”œâ”€â”€ DeepSeek/                               # DeepSeek-generated scripts (5 independent runs)
â”‚   â”‚   â”œâ”€â”€ Gemini/                                 # Gemini-generated scripts (5 independent runs)
â”‚   â”‚   â””â”€â”€ GPT/                                    # ChatGPT-generated scripts (5 independent runs)
â”‚   â””â”€â”€ Text
â”‚       â”œâ”€â”€ Human_Analysis.ipynb                    # Human analystâ€™s sentiment analysis workflow
â”‚       â”œâ”€â”€ DeepSeek/                               # DeepSeek-generated text analysis scripts
â”‚       â”œâ”€â”€ Gemini/                                 # Gemini-generated text analysis scripts
â”‚       â””â”€â”€ GPT/                                    # ChatGPT-generated text analysis scripts
â”œâ”€â”€ Data
â”‚   â”œâ”€â”€ diabetes_dataset.csv                        # Clean dataset used for tabular experiments
â”‚   â”œâ”€â”€ diabetes_dataset_nan.csv                    # Version with missing values for imputation tests
â”‚   â””â”€â”€ SemEval2017-task4-dev.subtask-A.english.INPUT.csv  # Dataset for sentiment analysis (text experiments)
â””â”€â”€ Prompts
    â”œâ”€â”€ Prompt_TabularData.txt                      # Prompt template used for the tabular data experiment
    â””â”€â”€ Prompt_TextData.txt                         # Prompt template used for the text-based experiment
```

---

## ğŸ¯ Objectives

1. **Quality of Outcomes (RQ2)**  
   Evaluate the clarity, relevance, completeness, and correctness of AI-generated analyses.  
   â†’ Quantitative and qualitative scores were assigned based on reproducibility and methodological soundness.

2. **Methodological Bias (RQ3)**  
   Examine how different AI tools approach critical analytic decisions (e.g., imputation strategy, variable encoding, visualization design).

3. **Cross-Domain Generalization (RQ1 & RQ4)**  
   Compare model performance between *tabular* and *text* datasets to assess robustness and domain transfer.

---

## ğŸ§© How to Reproduce

### 1. Requirements
Make sure you have the following installed:
```bash
python >= 3.10
jupyter
pandas
numpy
matplotlib
seaborn
scikit-learn
```

### 2. Run the Human Baselines
```bash
jupyter notebook Codes/Tabular/02_human_analysis.ipynb
jupyter notebook Codes/Text/Human_Analysis.ipynb
```

### 3. Run AI-Generated Analyses
Each AI model has 3â€“5 independent runs per domain:
```bash
python Codes/Tabular/Gemini/Gemini1.py
python Codes/Text/GPT/ChatGPT3.py
```

---

## ğŸ“Š Datasets

| Dataset | Type | Description |
|----------|------|-------------|
| `diabetes_dataset.csv` | Tabular | Medical dataset with health indicators (e.g., BMI, glucose) used for predictive modeling. |
| `diabetes_dataset_nan.csv` | Tabular | Version with synthetic missing values for imputation evaluation. |
| `SemEval2017-task4-dev.subtask-A.english.INPUT.csv` | Text | Dataset for sentiment classification (positive/neutral/negative). |

---

## ğŸ§  Models Compared

| Model | Provider | Runs | Domains Tested |
|--------|-----------|-------|----------------|
| **Gemini** | Google DeepMind | 5 (Tabular), 3 (Text) | Tabular + Text |
| **ChatGPT** | OpenAI | 5 (Tabular), 3 (Text) | Tabular + Text |
| **DeepSeek** | DeepSeek AI | 5 (Tabular), 3 (Text) | Tabular + Text |
| **Human Analyst** | Baseline | 1 | Tabular + Text |

---

## ğŸ§¾ Evaluation Criteria

Each generated analysis was evaluated on:
- **Relevance** â€” Are plots and metrics appropriate for the task?  
- **Clarity** â€” Is the reasoning clear and interpretable?  
- **Completeness** â€” Are all necessary analytical steps covered?  
- **Correctness** â€” Is the code runnable and logically sound?

---

## ğŸ–¼ï¸ Results Overview

Key findings (see Figures and Tables in the report):
- Gemini achieved the highest overall clarity and relevance.
- ChatGPT produced complete pipelines but suffered from frequent code errors.
- DeepSeek was generally runnable but conceptually inconsistent.
- All AI tools failed to detect hidden data issues (e.g., invalid BMI values = 0), defaulting to overly simple imputation strategies.

---

## ğŸ“š Citation

If you use this repository in your research, please cite it as:

```text
Huerta Moncho, A. (2025). AI-Driven Data Analysis Benchmark: 
Comparing Gemini, ChatGPT, and DeepSeek to Human Analysts.
```

---

## ğŸ§‘â€ğŸ’» Author


**Amadeo Huerta Moncho**  
MSc Intelligent Interactive Systems â€” Universitat Pompeu Fabra  
 â€¢ [GitHub](https://github.com/amadeohuerta)

**Sandra JimÃ©nez Vargas**
MSc Intelligent Interactive Systems â€” Universitat Pompeu Fabra  
 â€¢ [GitHub](https://github.com/sandrajivar)

**Jone Rivas Azpiazu**
MSc Intelligent Interactive Systems â€” Universitat Pompeu Fabra  
 â€¢ [GitHub](https://github.com/sta05)


